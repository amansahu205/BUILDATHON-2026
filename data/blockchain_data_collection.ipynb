{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348b73ed",
   "metadata": {},
   "source": [
    "# ğŸš€ VoiceFlow Intelligence â€” Comprehensive Blockchain Data Collection\n",
    "### NYU Buildathon 2026 | Feb 20â€“22 | Team: Nikhil, Aman, Dhanush\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ NOTEBOOK OVERVIEW\n",
    "\n",
    "This notebook collects **~25 GB of encyclopedic blockchain/crypto data** to power the **VoiceFlow Intelligence** RAG pipeline and fine-tuning datasets.  \n",
    "Run it **before the hackathon starts** and store the output folder on a USB drive.\n",
    "\n",
    "### Why Pre-Collect?\n",
    "- â±ï¸ Zero time wasted on data scraping during the event\n",
    "- ğŸ† Encyclopedic domain knowledge impresses judges and wins sponsor prizes  \n",
    "- ğŸ“¡ API rate limits will throttle you live â€” avoid this entirely\n",
    "- ğŸ¯ Pre-built RAG index = 8â€“12 extra hours for coding & polish\n",
    "\n",
    "### Prize Strategy\n",
    "| Data Category | Sponsor Unlocked | Prize Value |\n",
    "|---|---|---|\n",
    "| Binance OHLCV + technical indicators | TrueMarkets + FinTech track | $500â€“$1 000 |\n",
    "| CoinGecko 13 000-coin profiles | Databricks analytics showcase | $3 000 |\n",
    "| DeFi Llama protocols + yield pools | Databricks ML + FinTech | $3 000 |\n",
    "| OpenAssistant conversations | ElevenLabs voice dialogue | $7 920 |\n",
    "| ArXiv whitepapers + Wikipedia | Technical depth / Best Overall | $2 000 |\n",
    "\n",
    "### Sections\n",
    "1. Environment setup  \n",
    "2. Binance OHLCV price data (~5 GB)  \n",
    "3. CoinGecko encyclopedia (~2.5 GB)  \n",
    "4. DeFi ecosystem via DeFi Llama (~2.2 GB)  \n",
    "5. Protocol whitepapers â€” ArXiv + Wikipedia (~5.5 GB)  \n",
    "6. Regulatory & TrueMarkets docs (~1.1 GB)  \n",
    "7. Crypto news archives (~4 GB)  \n",
    "8. Conversation training data â€” OpenAssistant (~1.5 GB)  \n",
    "9. NFT & Web3 data (~3 GB)  \n",
    "10. TrueMarkets live API client  \n",
    "11. Build ChromaDB RAG index  \n",
    "12. Validation report & cheat sheet  \n",
    "\n",
    "âš ï¸ **Estimated total time:** 8â€“10 h | **Estimated size:** ~25 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae6210",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 â€” Environment Setup\n",
    "\n",
    "Install every library upfront so nothing breaks at 2 AM during the hackathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a56d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 1.1 â€” Install Dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# One-shot install of every library used across all sections.\n",
    "\n",
    "!pip install -q \\\n",
    "    requests pandas numpy tqdm \\\n",
    "    python-binance pycoingecko \\\n",
    "    arxiv wikipedia-api \\\n",
    "    datasets beautifulsoup4 lxml \\\n",
    "    gitpython \\\n",
    "    chromadb sentence-transformers \\\n",
    "    PyPDF2 aiohttp websockets nest_asyncio \\\n",
    "    python-dotenv tenacity \\\n",
    "    pyarrow fastparquet\n",
    "\n",
    "print(\"âœ… All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 1.2 â€” Global Config & Directory Structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, time, logging, hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€ Configure these before running â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "USB_MODE = False                         # Set True to write directly to USB\n",
    "USB_PATH = \"/Volumes/HACKATHON\"          # macOS USB path\n",
    "BASE_DIR = USB_PATH if USB_MODE else \"./hackathon_data\"\n",
    "\n",
    "DIRS = {\n",
    "    \"prices\":       f\"{BASE_DIR}/01_market_prices\",\n",
    "    \"coingecko\":    f\"{BASE_DIR}/02_coingecko_encyclopedia\",\n",
    "    \"defi\":         f\"{BASE_DIR}/03_defi_ecosystem\",\n",
    "    \"protocols\":    f\"{BASE_DIR}/04_protocol_whitepapers\",\n",
    "    \"regulatory\":   f\"{BASE_DIR}/05_regulatory_docs\",\n",
    "    \"news\":         f\"{BASE_DIR}/06_crypto_news\",\n",
    "    \"conversation\": f\"{BASE_DIR}/07_conversation_training\",\n",
    "    \"nft\":          f\"{BASE_DIR}/08_nft_web3\",\n",
    "    \"truemarkets\":  f\"{BASE_DIR}/09_truemarkets_live\",\n",
    "    \"rag_index\":    f\"{BASE_DIR}/10_rag_chromadb\",\n",
    "    \"logs\":         f\"{BASE_DIR}/logs\",\n",
    "}\n",
    "\n",
    "for p in DIRS.values():\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(f\"{DIRS['logs']}/collection.log\"),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "log = logging.getLogger(\"voiceflow\")\n",
    "\n",
    "# â”€â”€ API keys (fill in before running) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Binance:   https://www.binance.us/account/api  (optional â€” public endpoints work without)\n",
    "# CoinGecko: https://www.coingecko.com/en/api    (optional â€” free tier is enough)\n",
    "BINANCE_API_KEY    = os.getenv(\"BINANCE_API_KEY\",    \"\")\n",
    "BINANCE_API_SECRET = os.getenv(\"BINANCE_API_SECRET\", \"\")\n",
    "COINGECKO_API_KEY  = os.getenv(\"COINGECKO_API_KEY\",  \"\")\n",
    "\n",
    "# â”€â”€ Manifest: track every file we collect â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MANIFEST_PATH = f\"{BASE_DIR}/manifest.json\"\n",
    "manifest = {\"started_at\": datetime.now().isoformat(), \"sections\": {}}\n",
    "\n",
    "def save_manifest():\n",
    "    manifest[\"updated_at\"] = datetime.now().isoformat()\n",
    "    with open(MANIFEST_PATH, \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "def file_size_mb(path):\n",
    "    try:    return round(os.path.getsize(path) / 1024 / 1024, 2)\n",
    "    except: return 0\n",
    "\n",
    "def dir_size_mb(path):\n",
    "    total = 0\n",
    "    for dp, _, fnames in os.walk(path):\n",
    "        for fn in fnames:\n",
    "            total += os.path.getsize(os.path.join(dp, fn))\n",
    "    return round(total / 1024 / 1024, 2)\n",
    "\n",
    "log.info(f\"Output directory: {BASE_DIR}\")\n",
    "print(f\"âœ… Config ready â€” writing to: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bd225",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 â€” Binance OHLCV Price Data\n",
    "\n",
    "### Why this data?\n",
    "| Use case | Detail |\n",
    "|---|---|\n",
    "| **Databricks prize** | Store as Delta Lake Parquet; track feature engineering in MLflow |\n",
    "| **Nemotron** | Feed structured OHLCV + indicators â€” math-capable model handles RSI/MACD/BB calc |\n",
    "| **TrueMarkets** | Historical context so voice agent says *\"BTC is up 3.2% today, last at $87 420\"* |\n",
    "| **ElevenLabs** | Factually grounded price references make voice responses sound authoritative |\n",
    "\n",
    "### What we collect\n",
    "10 major pairs Ã— 365 days Ã— 1-minute candles â‰ˆ **~5.25 million rows** saved as snappy-compressed Parquet files.  \n",
    "Parquet is 5â€“10Ã— smaller than CSV and loads instantly in Databricks Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 2.1 â€” Fetch Binance 1-Minute OHLCV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Binance public klines endpoint â€” NO API key required.\n",
    "# We page through 1 000-candle chunks until the full year is downloaded.\n",
    "#\n",
    "# Column meanings:\n",
    "#   open/high/low/close â†’ price in quote asset (USDT)\n",
    "#   volume             â†’ base asset volume\n",
    "#   quote_volume       â†’ notional traded (â‰ˆ USD value)\n",
    "#   trades             â†’ number of individual trades in that minute\n",
    "#   taker_buy_base/quote â†’ volume bought by the aggressive (taker) side\n",
    "#\n",
    "# WHY 1-MINUTE CANDLES?\n",
    "#   Granular enough to calculate any higher-timeframe indicator.\n",
    "#   A 1-hour candle = resample 60 rows. A daily candle = 1 440 rows.\n",
    "#   Gives Databricks a real workload to showcase parallelism.\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "BINANCE_BASE = \"https://api.binance.us/api/v3\"\n",
    "\n",
    "SYMBOLS   = [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"XRPUSDT\",\n",
    "             \"ADAUSDT\",\"AVAXUSDT\",\"MATICUSDT\",\"LINKUSDT\",\"UNIUSDT\"]\n",
    "INTERVAL  = \"1m\"\n",
    "DAYS_BACK = 365\n",
    "LIMIT     = 1000\n",
    "\n",
    "def fetch_klines(symbol, interval, start_ms, end_ms):\n",
    "    r = requests.get(\n",
    "        f\"{BINANCE_BASE}/klines\",\n",
    "        params={\"symbol\": symbol, \"interval\": interval,\n",
    "                \"startTime\": start_ms, \"endTime\": end_ms, \"limit\": LIMIT},\n",
    "        timeout=30,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def collect_symbol_ohlcv(symbol):\n",
    "    out = f\"{DIRS['prices']}/{symbol}_{INTERVAL}_{DAYS_BACK}d.parquet\"\n",
    "    if os.path.exists(out):\n",
    "        log.info(f\"  Skipping {symbol} â€” already collected\")\n",
    "        return out\n",
    "\n",
    "    end_ms   = int(datetime.utcnow().timestamp() * 1000)\n",
    "    start_ms = int((datetime.utcnow() - timedelta(days=DAYS_BACK)).timestamp() * 1000)\n",
    "    candles, cursor = [], start_ms\n",
    "\n",
    "    with tqdm(total=DAYS_BACK * 1440, desc=f\"  {symbol}\", unit=\"candles\") as pbar:\n",
    "        while cursor < end_ms:\n",
    "            try:\n",
    "                batch = fetch_klines(symbol, INTERVAL, cursor, end_ms)\n",
    "            except requests.RequestException as e:\n",
    "                log.warning(f\"  Rate limited ({symbol}): {e}. Retrying in 10 s...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            if not batch:\n",
    "                break\n",
    "            candles.extend(batch)\n",
    "            cursor = batch[-1][6] + 1   # close_time of last candle + 1 ms\n",
    "            pbar.update(len(batch))\n",
    "            time.sleep(0.12)            # â‰ˆ 8 req/s â€” well under Binance 1 200/min limit\n",
    "\n",
    "    cols = [\"open_time\",\"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "            \"close_time\",\"quote_volume\",\"trades\",\n",
    "            \"taker_buy_base\",\"taker_buy_quote\",\"_ignore\"]\n",
    "    df = pd.DataFrame(candles, columns=cols)\n",
    "    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\n",
    "              \"taker_buy_base\",\"taker_buy_quote\"]:\n",
    "        df[c] = df[c].astype(float)\n",
    "    df[\"trades\"]   = df[\"trades\"].astype(int)\n",
    "    df[\"open_dt\"]  = pd.to_datetime(df[\"open_time\"],  unit=\"ms\", utc=True)\n",
    "    df[\"close_dt\"] = pd.to_datetime(df[\"close_time\"], unit=\"ms\", utc=True)\n",
    "    df[\"symbol\"]   = symbol\n",
    "    df.drop(columns=[\"_ignore\"], inplace=True)\n",
    "    df.to_parquet(out, index=False, compression=\"snappy\")\n",
    "    log.info(f\"  âœ… {symbol}: {len(df):,} candles â†’ {file_size_mb(out)} MB\")\n",
    "    return out\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SECTION 2: Binance OHLCV collection\")\n",
    "print(\"=\" * 60)\n",
    "price_paths = [collect_symbol_ohlcv(s) for s in SYMBOLS]\n",
    "manifest[\"sections\"][\"prices\"] = {\"symbols\": SYMBOLS,\n",
    "                                   \"total_mb\": dir_size_mb(DIRS[\"prices\"])}\n",
    "save_manifest()\n",
    "print(f\"\\nâœ… Price data complete â€” {dir_size_mb(DIRS['prices'])} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 2.2 â€” Technical Indicators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Pre-computing indicators has two benefits:\n",
    "#   1. Nemotron gets LABELLED features (\"RSI(14)=68.4\") not raw numbers.\n",
    "#      This activates its multi-step mathematical reasoning capability.\n",
    "#   2. Databricks demo: register a feature store in Unity Catalog and\n",
    "#      track a feature engineering job in MLflow Experiments.\n",
    "#\n",
    "# INDICATOR REFERENCE:\n",
    "#   RSI(14)       â†’ 0â€“100 momentum oscillator; >70 = overbought, <30 = oversold\n",
    "#   SMA(20)       â†’ Simple 20-period baseline; acts as dynamic support/resistance\n",
    "#   EMA(12/26)    â†’ Exponential MAs that weight recent prices more heavily\n",
    "#   MACD          â†’ EMA(12)â€“EMA(26); crossover = momentum shift signal\n",
    "#   MACD signal   â†’ EMA(9) of MACD; generates buy/sell crossover signals\n",
    "#   MACD hist     â†’ MACDâ€“signal; histogram shows divergence strength\n",
    "#   BB upper/lowerâ†’ SMA(20) Â± 2Ïƒ; price near upper = expensive, near lower = cheap\n",
    "#   BB width      â†’ Normalised band width; spikes = high volatility regimes\n",
    "#   VWAP(60m)     â†’ Volume-weighted average price over last hour;\n",
    "#                   TrueMarkets uses VWAP for institutional order routing\n",
    "\n",
    "def add_indicators(df):\n",
    "    c, v = df[\"close\"], df[\"volume\"]\n",
    "\n",
    "    delta = c.diff()\n",
    "    gain  = delta.clip(lower=0).rolling(14).mean()\n",
    "    loss  = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "    df[\"rsi_14\"]      = 100 - 100 / (1 + gain / loss.replace(0, 1e-10))\n",
    "\n",
    "    df[\"sma_20\"]      = c.rolling(20).mean()\n",
    "    df[\"ema_12\"]      = c.ewm(span=12, adjust=False).mean()\n",
    "    df[\"ema_26\"]      = c.ewm(span=26, adjust=False).mean()\n",
    "    df[\"macd\"]        = df[\"ema_12\"] - df[\"ema_26\"]\n",
    "    df[\"macd_signal\"] = df[\"macd\"].ewm(span=9, adjust=False).mean()\n",
    "    df[\"macd_hist\"]   = df[\"macd\"] - df[\"macd_signal\"]\n",
    "\n",
    "    std = c.rolling(20).std()\n",
    "    df[\"bb_upper\"]    = df[\"sma_20\"] + 2 * std\n",
    "    df[\"bb_lower\"]    = df[\"sma_20\"] - 2 * std\n",
    "    df[\"bb_width\"]    = (df[\"bb_upper\"] - df[\"bb_lower\"]) / df[\"sma_20\"]\n",
    "\n",
    "    tp = (df[\"high\"] + df[\"low\"] + c) / 3\n",
    "    df[\"vwap_60m\"]    = (tp * v).rolling(60).sum() / v.rolling(60).sum()\n",
    "\n",
    "    df[\"pct_1m\"]  = c.pct_change(1)    * 100\n",
    "    df[\"pct_1h\"]  = c.pct_change(60)   * 100\n",
    "    df[\"pct_1d\"]  = c.pct_change(1440) * 100\n",
    "    return df\n",
    "\n",
    "for sym in SYMBOLS:\n",
    "    src = f\"{DIRS['prices']}/{sym}_{INTERVAL}_{DAYS_BACK}d.parquet\"\n",
    "    if not os.path.exists(src):\n",
    "        print(f\"  âš ï¸  {sym} raw file missing â€” run Cell 2.1 first\")\n",
    "        continue\n",
    "    df  = pd.read_parquet(src)\n",
    "    df  = add_indicators(df)\n",
    "    out = f\"{DIRS['prices']}/{sym}_features.parquet\"\n",
    "    df.to_parquet(out, index=False, compression=\"snappy\")\n",
    "    print(f\"  âœ… {sym} features â†’ {file_size_mb(out)} MB\")\n",
    "\n",
    "print(\"\\nâœ… Technical indicators complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507942ec",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 â€” CoinGecko Encyclopedia (13 000+ Coins)\n",
    "\n",
    "### Why this data?\n",
    "This is the **encyclopedic layer** â€” what separates a toy demo from a production-grade agent.\n",
    "\n",
    "When a judge asks *\"Tell me about Render Network\"* (coin #847) or *\"What consensus does Fantom use?\"*, the agent needs this.\n",
    "\n",
    "### What we collect per coin\n",
    "- Full text description (technology, use case, team)\n",
    "- Category tags (DeFi, NFT, L1, L2, Gaming, PoW, PoSâ€¦)\n",
    "- Market data: cap, volume, ATH, ATL, supply\n",
    "- Developer stats: GitHub commits, contributors, issues\n",
    "- Social stats: Twitter followers, Reddit subscribers\n",
    "- Exchange listings and trading pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 3.1 â€” CoinGecko: Full Market List â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Strategy: page through /coins/markets (250 per page) for a lightweight\n",
    "# market-data snapshot of every listed coin, then call /coins/{id} for\n",
    "# the richer profile data we need for RAG.\n",
    "#\n",
    "# Rate limits:\n",
    "#   Free tier â‰ˆ 10â€“30 calls/min  â†’ sleep 6 s between calls\n",
    "#   Pro tier  â‰ˆ 500 calls/min   â†’ sleep 0.12 s\n",
    "# We sleep conservatively to avoid 429s during overnight collection.\n",
    "\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "\n",
    "cg = CoinGeckoAPI(api_key=COINGECKO_API_KEY if COINGECKO_API_KEY else None)\n",
    "\n",
    "def get_all_coins_market_data():\n",
    "    out = f\"{DIRS['coingecko']}/all_coins_market.parquet\"\n",
    "    if os.path.exists(out):\n",
    "        log.info(\"  Skipping CoinGecko market list â€” already collected\")\n",
    "        return pd.read_parquet(out)\n",
    "\n",
    "    all_coins, page = [], 1\n",
    "    while True:\n",
    "        try:\n",
    "            batch = cg.get_coins_markets(\n",
    "                vs_currency=\"usd\", order=\"market_cap_desc\",\n",
    "                per_page=250, page=page, sparkline=False,\n",
    "                price_change_percentage=\"24h,7d,30d\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log.warning(f\"  Page {page} error: {e}. Sleeping 30 s...\")\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "        if not batch:\n",
    "            break\n",
    "        all_coins.extend(batch)\n",
    "        print(f\"  Page {page}: +{len(batch)} coins (total {len(all_coins)})\")\n",
    "        page += 1\n",
    "        time.sleep(6)\n",
    "\n",
    "    df = pd.DataFrame(all_coins)\n",
    "    df.to_parquet(out, index=False, compression=\"snappy\")\n",
    "    print(f\"\\n  âœ… {len(df)} coins saved\")\n",
    "    return df\n",
    "\n",
    "coins_df = get_all_coins_market_data()\n",
    "print(f\"Total coins collected: {len(coins_df):,}\")\n",
    "print(coins_df[[\"name\",\"symbol\",\"market_cap_rank\",\"current_price\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 3.2 â€” CoinGecko: Rich Coin Profiles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# For each coin we store the FULL profile which becomes a RAG document:\n",
    "#\n",
    "#   title:   \"Bitcoin (BTC)\"\n",
    "#   text:    \"Bitcoin is a decentralised digital currency...\n",
    "#             Categories: Cryptocurrency, PoW, Store of Value\n",
    "#             Market Cap: $1.7 T | Rank: #1 | ATH: $108 000\n",
    "#             Algorithm: SHA-256 | Block time: 10 min\"\n",
    "#\n",
    "# We use a done_ids.txt checkpoint file so the run is fully resumable\n",
    "# if your connection drops or rate limit hits mid-collection.\n",
    "\n",
    "def fetch_coin_profile(coin_id):\n",
    "    try:\n",
    "        d = cg.get_coin_by_id(\n",
    "            coin_id, localization=False, tickers=False,\n",
    "            market_data=True, community_data=True,\n",
    "            developer_data=True, sparkline=False,\n",
    "        )\n",
    "        return {\n",
    "            \"id\":            coin_id,\n",
    "            \"name\":          d.get(\"name\"),\n",
    "            \"symbol\":        d.get(\"symbol\"),\n",
    "            \"description\":   (d.get(\"description\") or {}).get(\"en\", \"\")[:2000],\n",
    "            \"categories\":    d.get(\"categories\", []),\n",
    "            \"homepage\":      (d.get(\"links\") or {}).get(\"homepage\", [None])[0],\n",
    "            \"github\":        (d.get(\"links\") or {}).get(\"repos_url\", {}).get(\"github\", []),\n",
    "            \"market_cap_usd\":    (d.get(\"market_data\") or {}).get(\"market_cap\", {}).get(\"usd\"),\n",
    "            \"current_price_usd\": (d.get(\"market_data\") or {}).get(\"current_price\", {}).get(\"usd\"),\n",
    "            \"ath_usd\":           (d.get(\"market_data\") or {}).get(\"ath\", {}).get(\"usd\"),\n",
    "            \"genesis_date\":      d.get(\"genesis_date\"),\n",
    "            \"hashing_algorithm\": d.get(\"hashing_algorithm\"),\n",
    "            \"block_time_minutes\":d.get(\"block_time_in_minutes\"),\n",
    "            \"total_supply\":      (d.get(\"market_data\") or {}).get(\"total_supply\"),\n",
    "            \"circulating_supply\":(d.get(\"market_data\") or {}).get(\"circulating_supply\"),\n",
    "            \"developer_score\":   d.get(\"developer_score\"),\n",
    "            \"community_score\":   d.get(\"community_score\"),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log.warning(f\"  Could not fetch {coin_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def collect_coin_profiles(coins_df, max_coins=3000):\n",
    "    out_path  = f\"{DIRS['coingecko']}/coin_profiles.jsonl\"\n",
    "    done_path = f\"{DIRS['coingecko']}/done_ids.txt\"\n",
    "\n",
    "    done_ids = set()\n",
    "    if os.path.exists(done_path):\n",
    "        done_ids = set(open(done_path).read().splitlines())\n",
    "        print(f\"  Resuming: {len(done_ids)} profiles cached\")\n",
    "\n",
    "    targets = coins_df.sort_values(\"market_cap_rank\").head(max_coins)[\"id\"].tolist()\n",
    "    remaining = [cid for cid in targets if cid not in done_ids]\n",
    "    print(f\"  Fetching {len(remaining)} profiles...\")\n",
    "\n",
    "    with open(out_path, \"a\") as out_f, open(done_path, \"a\") as done_f:\n",
    "        for cid in tqdm(remaining, desc=\"  Coin profiles\"):\n",
    "            profile = fetch_coin_profile(cid)\n",
    "            if profile:\n",
    "                out_f.write(json.dumps(profile) + \"\\n\")\n",
    "                done_f.write(cid + \"\\n\")\n",
    "            time.sleep(6)   # Free-tier rate limit\n",
    "\n",
    "    total = len(done_ids) + len(remaining)\n",
    "    print(f\"  âœ… {total} profiles saved\")\n",
    "\n",
    "collect_coin_profiles(coins_df, max_coins=3000)\n",
    "manifest[\"sections\"][\"coingecko\"] = {\"total_mb\": dir_size_mb(DIRS[\"coingecko\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12ea4b",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 â€” DeFi Ecosystem (DeFi Llama)\n",
    "\n",
    "### Why this data?\n",
    "DeFi is the most technically complex domain judges probe on.  \n",
    "DeFi Llama has a **completely free, no-auth API** covering 3 000+ protocols.\n",
    "\n",
    "Voice agent capabilities this unlocks:\n",
    "- *\"What's the TVL on Aave vs Compound?\"* â†’ real numbers\n",
    "- *\"Which chain has the most DeFi activity?\"* â†’ chain breakdown\n",
    "- *\"What's the best USDC yield right now?\"* â†’ live pool APYs\n",
    "\n",
    "### What we collect\n",
    "- All protocol metadata + TVL + category\n",
    "- Historical TVL time series for top 200 (Databricks time-series model)\n",
    "- Live yield pool APYs across every DeFi lending/liquidity protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 4.1 â€” DeFi Llama: Protocols + Yield Pools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DeFi Llama protocol record includes:\n",
    "#   name, slug, category (Dexes/Lending/CDP/Derivatives/Yield/Bridge/Stablecoinsâ€¦)\n",
    "#   tvl (USD), change_1h/1d/7d, chains (where it's deployed), description,\n",
    "#   github, twitter, forkedFrom (which protocol it's a fork of)\n",
    "#\n",
    "# forkedFrom is GOLD for the agent:\n",
    "#   \"SushiSwap is a fork of Uniswap V2, adding SUSHI token rewards...\"\n",
    "#\n",
    "# YIELD POOL DATA (from yields.llama.fi):\n",
    "#   pool, project, chain, symbol, tvlUsd, apy, apyBase, apyReward,\n",
    "#   underlyingTokens, rewardTokens, apyPct7D, apyPct30D\n",
    "#   â†’ agent can compare real-time yields across protocols\n",
    "\n",
    "LLAMA_BASE = \"https://api.llama.fi\"\n",
    "\n",
    "def collect_defi_protocols():\n",
    "    out = f\"{DIRS['defi']}/all_protocols.parquet\"\n",
    "    if os.path.exists(out):\n",
    "        log.info(\"  Skipping DeFi protocols â€” already collected\")\n",
    "        return pd.read_parquet(out)\n",
    "\n",
    "    resp = requests.get(f\"{LLAMA_BASE}/protocols\", timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    rows = [{\n",
    "        \"name\":       p.get(\"name\"),\n",
    "        \"slug\":       p.get(\"slug\"),\n",
    "        \"url\":        p.get(\"url\"),\n",
    "        \"description\":str(p.get(\"description\", \"\"))[:1000],\n",
    "        \"chain\":      p.get(\"chain\"),\n",
    "        \"chains\":     json.dumps(p.get(\"chains\", [])),\n",
    "        \"category\":   p.get(\"category\"),\n",
    "        \"tvl\":        p.get(\"tvl\", 0),\n",
    "        \"change_1h\":  p.get(\"change_1h\", 0),\n",
    "        \"change_1d\":  p.get(\"change_1d\", 0),\n",
    "        \"change_7d\":  p.get(\"change_7d\", 0),\n",
    "        \"github\":     json.dumps(p.get(\"github\", [])),\n",
    "        \"forkedFrom\": json.dumps(p.get(\"forkedFrom\", [])),\n",
    "        \"twitter\":    p.get(\"twitter\"),\n",
    "        \"mcap\":       p.get(\"mcap\"),\n",
    "    } for p in data]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_parquet(out, index=False, compression=\"snappy\")\n",
    "    print(f\"  âœ… {len(df)} protocols saved ({file_size_mb(out)} MB)\")\n",
    "    return df\n",
    "\n",
    "def collect_yield_pools():\n",
    "    out = f\"{DIRS['defi']}/yield_pools.parquet\"\n",
    "    if os.path.exists(out):\n",
    "        print(\"  Skipping yield pools â€” already collected\"); return\n",
    "    resp = requests.get(\"https://yields.llama.fi/pools\", timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    df = pd.DataFrame(resp.json().get(\"data\", []))\n",
    "    df.to_parquet(out, index=False, compression=\"snappy\")\n",
    "    print(f\"  âœ… {len(df)} yield pools saved ({file_size_mb(out)} MB)\")\n",
    "\n",
    "def collect_tvl_history(slug):\n",
    "    out = f\"{DIRS['defi']}/tvl_history/{slug}.json\"\n",
    "    Path(f\"{DIRS['defi']}/tvl_history\").mkdir(exist_ok=True)\n",
    "    if os.path.exists(out): return\n",
    "    try:\n",
    "        r = requests.get(f\"{LLAMA_BASE}/protocol/{slug}\", timeout=15)\n",
    "        r.raise_for_status()\n",
    "        d = r.json()\n",
    "        with open(out, \"w\") as f:\n",
    "            json.dump({\"slug\": slug, \"name\": d.get(\"name\"),\n",
    "                       \"category\": d.get(\"category\"),\n",
    "                       \"tvl\": d.get(\"tvl\", []),\n",
    "                       \"chainTvls\": d.get(\"chainTvls\", {})}, f)\n",
    "    except Exception as e:\n",
    "        log.warning(f\"  TVL history error for {slug}: {e}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SECTION 4: DeFi Ecosystem\")\n",
    "print(\"=\" * 60)\n",
    "protocols_df = collect_defi_protocols()\n",
    "collect_yield_pools()\n",
    "\n",
    "top200 = protocols_df.nlargest(200, \"tvl\")[\"slug\"].dropna().tolist()\n",
    "print(f\"\\nCollecting TVL history for top 200 protocols...\")\n",
    "for slug in tqdm(top200, desc=\"  TVL history\"):\n",
    "    collect_tvl_history(slug)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "manifest[\"sections\"][\"defi\"] = {\"protocols\": len(protocols_df),\n",
    "                                 \"total_mb\": dir_size_mb(DIRS[\"defi\"])}\n",
    "save_manifest()\n",
    "print(f\"\\nâœ… DeFi data complete â€” {dir_size_mb(DIRS['defi'])} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf098e52",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 â€” Protocol Whitepapers & Technical Documentation\n",
    "\n",
    "### Why this data?\n",
    "Technical credibility is what separates 1st place from 2nd.  \n",
    "Judges drill into specifics: *\"How does Solana's Proof of History work?\"* â€” if the agent stumbles, you lose.\n",
    "\n",
    "### Sources\n",
    "| Source | What it provides |\n",
    "|---|---|\n",
    "| **ArXiv** | Peer-reviewed papers on consensus, ZK proofs, AMMs, MEV, bridges |\n",
    "| **Wikipedia** | Accessible explanations; great for ElevenLabs voice synthesis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b333229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 5.1 â€” ArXiv Academic Papers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 17 targeted queries Ã— 50 results each â‰ˆ 850 papers.\n",
    "# We store abstract + metadata â€” sufficient for RAG without downloading full PDFs.\n",
    "#\n",
    "# Query design: specific technical terms â†’ high-precision results\n",
    "# e.g. \"optimistic rollup fraud proof layer 2 scaling\" targets the exact\n",
    "# concept rather than generic \"ethereum scaling\" (which returns too much noise)\n",
    "\n",
    "import arxiv\n",
    "\n",
    "ARXIV_QUERIES = [\n",
    "    \"blockchain consensus mechanism proof of stake\",\n",
    "    \"Bitcoin proof of work nakamoto consensus\",\n",
    "    \"Ethereum smart contract EVM execution\",\n",
    "    \"optimistic rollup fraud proof layer 2 scaling\",\n",
    "    \"zero knowledge proof zkSNARK zkSTARK blockchain\",\n",
    "    \"state channel payment channel lightning network\",\n",
    "    \"automated market maker AMM DEX constant product formula\",\n",
    "    \"DeFi lending protocol liquidation overcollateralization\",\n",
    "    \"maximal extractable value MEV flashbot blockchain\",\n",
    "    \"stablecoin algorithmic collateralized mechanism\",\n",
    "    \"elliptic curve cryptography digital signature blockchain\",\n",
    "    \"hash function SHA256 merkle tree blockchain\",\n",
    "    \"cross chain bridge interoperability protocol\",\n",
    "    \"blockchain oracle price feed manipulation\",\n",
    "    \"NFT non-fungible token smart contract ERC-721\",\n",
    "    \"DAO governance voting mechanism token weighted\",\n",
    "    \"tokenomics mechanism design crypto incentives\",\n",
    "]\n",
    "\n",
    "def collect_arxiv_papers():\n",
    "    out = f\"{DIRS['protocols']}/arxiv_papers.jsonl\"\n",
    "    seen = set()\n",
    "    if os.path.exists(out):\n",
    "        with open(out) as f:\n",
    "            seen = {json.loads(l)[\"id\"] for l in f}\n",
    "        print(f\"  Resuming: {len(seen)} papers cached\")\n",
    "\n",
    "    new_count = 0\n",
    "    with open(out, \"a\") as f:\n",
    "        for q in tqdm(ARXIV_QUERIES, desc=\"  ArXiv queries\"):\n",
    "            search = arxiv.Search(query=q, max_results=50,\n",
    "                                  sort_by=arxiv.SortCriterion.Relevance)\n",
    "            for p in search.results():\n",
    "                pid = p.entry_id.split(\"/\")[-1]\n",
    "                if pid in seen: continue\n",
    "                f.write(json.dumps({\n",
    "                    \"id\":        pid,\n",
    "                    \"title\":     p.title,\n",
    "                    \"abstract\":  p.summary,\n",
    "                    \"authors\":   [str(a) for a in p.authors],\n",
    "                    \"published\": p.published.isoformat() if p.published else None,\n",
    "                    \"categories\":p.categories,\n",
    "                    \"url\":       p.entry_id,\n",
    "                    \"query\":     q,\n",
    "                }) + \"\\n\")\n",
    "                seen.add(pid)\n",
    "                new_count += 1\n",
    "            time.sleep(3)   # ArXiv polite rate limit\n",
    "\n",
    "    print(f\"  âœ… {new_count} new papers saved (total {len(seen)})\")\n",
    "    return out\n",
    "\n",
    "arxiv_path = collect_arxiv_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 5.2 â€” Wikipedia Crypto Articles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Wikipedia gives the agent:\n",
    "#   1. Accessible explanations â€” ideal for ElevenLabs voice (natural sentences,\n",
    "#      not bullet-point jargon from whitepapers)\n",
    "#   2. Historical timelines â€” FTX collapse, 2017 ICO mania, Mt. Gox\n",
    "#   3. Cross-concept links â€” agent can chain from \"DeFi\" â†’ \"AMM\" â†’ \"Uniswap\"\n",
    "#\n",
    "# We cap each article at 50 000 chars (â‰ˆ 12 500 tokens) â€” more than enough\n",
    "# for RAG retrieval while keeping storage manageable.\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "WIKI_TOPICS = [\n",
    "    \"Cryptocurrency\",\"Blockchain\",\"Bitcoin\",\"Ethereum\",\"Smart_contract\",\n",
    "    \"Decentralized_finance\",\"Non-fungible_token\",\"Web3\",\n",
    "    \"Decentralized_autonomous_organization\",\n",
    "    \"Proof_of_work\",\"Proof_of_stake\",\"Delegated_proof_of_stake\",\n",
    "    \"Byzantine_fault\",\"Merkle_tree\",\"Hash_function\",\"SHA-2\",\n",
    "    \"Elliptic-curve_cryptography\",\"Zero-knowledge_proof\",\n",
    "    \"Lightning_Network\",\"Ethereum_2.0\",\n",
    "    \"Solana_(blockchain)\",\"Avalanche_(blockchain_platform)\",\n",
    "    \"Cardano_(blockchain_platform)\",\"Polkadot_(blockchain_platform)\",\n",
    "    \"Cosmos_(blockchain)\",\"Ripple_(payment_protocol)\",\n",
    "    \"Binance_Smart_Chain\",\"Polygon_(blockchain)\",\n",
    "    \"Uniswap\",\"Automated_market_maker\",\"Yield_farming\",\n",
    "    \"Liquidity_mining\",\"Decentralized_exchange\",\n",
    "    \"Stablecoin\",\"Tether_(cryptocurrency)\",\"USD_Coin\",\"Dai_(cryptocurrency)\",\n",
    "    \"Cryptocurrency_exchange\",\"Initial_coin_offering\",\"Crypto_winter\",\n",
    "    \"Bitcoin_halving\",\"Mt._Gox\",\"FTX_(company)\",\"Celsius_Network\",\n",
    "    \"Cryptocurrency_and_crime\",\"Bitcoin_in_El_Salvador\",\"Cryptocurrency_bubble\",\n",
    "]\n",
    "\n",
    "def collect_wikipedia():\n",
    "    out = f\"{DIRS['protocols']}/wikipedia_articles.jsonl\"\n",
    "    done = set()\n",
    "    if os.path.exists(out):\n",
    "        with open(out) as f:\n",
    "            done = {json.loads(l)[\"title\"] for l in f}\n",
    "        print(f\"  Resuming: {len(done)} articles cached\")\n",
    "\n",
    "    wiki = wikipediaapi.Wikipedia(\n",
    "        user_agent=\"VoiceFlowIntelligence/1.0 (NYU Buildathon 2026)\",\n",
    "        language=\"en\",\n",
    "    )\n",
    "    new_count = 0\n",
    "    with open(out, \"a\") as f:\n",
    "        for topic in tqdm(WIKI_TOPICS, desc=\"  Wikipedia\"):\n",
    "            if topic in done: continue\n",
    "            page = wiki.page(topic)\n",
    "            if not page.exists():\n",
    "                log.warning(f\"  Not found: {topic}\"); continue\n",
    "            f.write(json.dumps({\n",
    "                \"title\":     page.title,\n",
    "                \"summary\":   page.summary[:2000],\n",
    "                \"full_text\": page.text[:50000],\n",
    "                \"url\":       page.fullurl,\n",
    "                \"sections\":  [s.title for s in page.sections],\n",
    "            }) + \"\\n\")\n",
    "            new_count += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    print(f\"  âœ… {new_count} new Wikipedia articles saved\")\n",
    "    return out\n",
    "\n",
    "wiki_path = collect_wikipedia()\n",
    "manifest[\"sections\"][\"protocols\"] = {\"total_mb\": dir_size_mb(DIRS[\"protocols\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630fda0",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 â€” Regulatory & TrueMarkets Documentation\n",
    "\n",
    "### Why this data?\n",
    "TrueMarkets is a sponsor with a full Rule Book. The voice agent must answer:\n",
    "- *\"Why was my order rejected?\"* â†’ price band, min order value, tick size\n",
    "- *\"What's the self-trade prevention behaviour?\"* â†’ cancel-aggressive vs cancel-both\n",
    "- *\"Explain the DeFi Gateway order ingress rules\"* â†’ Appendix A\n",
    "\n",
    "We chunk the docs at ~1 000 chars with 100-char overlap so RAG retrieval hits the right paragraph, not a 10-page wall of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 6.1 â€” TrueMarkets Documentation Ingestion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# The 14 TrueMarkets .txt files in /mnt/project/ become RAG-ready chunks.\n",
    "#\n",
    "# CHUNKING STRATEGY â€” why it matters:\n",
    "#   Too large  â†’ retrieval returns a 5-page chapter; signal buried in noise\n",
    "#   Too small  â†’ retrieval returns half a sentence; no useful context\n",
    "#   ~1 000 chars with 100-char overlap â†’ retrieves exactly the paragraph\n",
    "#   the user asked about plus a little context on either side.\n",
    "#\n",
    "# METADATA tagging:\n",
    "#   Every chunk tagged doc_type=\"truemarkets_official\" so we can do\n",
    "#   source-filtered queries: \"search ONLY TrueMarkets docs for price band rules\"\n",
    "\n",
    "TRUE_MARKETS_DOCS = [\n",
    "    \"/mnt/project/About_True_Markets.txt\",\n",
    "    \"/mnt/project/Rule_Book.txt\",\n",
    "    \"/mnt/project/Price_Bands.txt\",\n",
    "    \"/mnt/project/Increment_Sizes.txt\",\n",
    "    \"/mnt/project/Exchange_Overview.txt\",\n",
    "    \"/mnt/project/Order_Entry.txt\",\n",
    "    \"/mnt/project/Market_Data.txt\",\n",
    "    \"/mnt/project/Drop_Copy.txt\",\n",
    "    \"/mnt/project/TrueX_FIX_API.txt\",\n",
    "    \"/mnt/project/TrueX_REST_API.txt\",\n",
    "    \"/mnt/project/TrueX_WebSocket_API.txt\",\n",
    "    \"/mnt/project/Common_Components.txt\",\n",
    "    \"/mnt/project/Administrative.txt\",\n",
    "    \"/mnt/project/DeFi_Service_API.txt\",\n",
    "]\n",
    "\n",
    "CHUNK_SIZE    = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "def ingest_truemarkets_docs():\n",
    "    out = f\"{DIRS['regulatory']}/truemarkets_docs.jsonl\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc_path in TRUE_MARKETS_DOCS:\n",
    "        if not os.path.exists(doc_path):\n",
    "            print(f\"  âš ï¸  Not found (expected in /mnt/project/): {doc_path}\")\n",
    "            continue\n",
    "        with open(doc_path, encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        doc_name   = Path(doc_path).stem\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if len(p.strip()) > 50]\n",
    "        current    = \"\"\n",
    "        doc_chunks = 0\n",
    "\n",
    "        for para in paragraphs:\n",
    "            if len(current) + len(para) < CHUNK_SIZE:\n",
    "                current += \"\\n\\n\" + para\n",
    "            else:\n",
    "                if current.strip():\n",
    "                    all_chunks.append({\"source\": doc_name,\n",
    "                                       \"text\": current.strip(),\n",
    "                                       \"doc_type\": \"truemarkets_official\"})\n",
    "                    doc_chunks += 1\n",
    "                current = current[-CHUNK_OVERLAP:] + \"\\n\\n\" + para\n",
    "\n",
    "        if current.strip():\n",
    "            all_chunks.append({\"source\": doc_name,\n",
    "                               \"text\": current.strip(),\n",
    "                               \"doc_type\": \"truemarkets_official\"})\n",
    "            doc_chunks += 1\n",
    "\n",
    "        print(f\"  âœ… {doc_name}: {len(paragraphs)} paragraphs â†’ {doc_chunks} chunks\")\n",
    "\n",
    "    with open(out, \"w\") as f:\n",
    "        for chunk in all_chunks:\n",
    "            f.write(json.dumps(chunk) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n  Total: {len(all_chunks)} chunks â†’ {file_size_mb(out)} MB\")\n",
    "    return out\n",
    "\n",
    "tm_path = ingest_truemarkets_docs()\n",
    "manifest[\"sections\"][\"regulatory\"] = {\"total_mb\": dir_size_mb(DIRS[\"regulatory\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c759886",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 â€” Crypto News Archives\n",
    "\n",
    "### Why this data?\n",
    "News enables sentiment-aware responses and event context:\n",
    "- *\"After the FTX collapse in Nov 2022, BTC dropped 25% in a week\"*\n",
    "- Databricks demo: train a headline sentiment classifier, track in MLflow\n",
    "\n",
    "CryptoPanic votes (`positive`, `negative`, `important`) give us human-labelled sentiment â€” no annotation work needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 7.1 â€” CryptoPanic News + Sentiment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CryptoPanic aggregates 100+ crypto media sources.\n",
    "# We pull 5 filters Ã— N pages â‰ˆ 20 000 articles with vote-based sentiment labels.\n",
    "#\n",
    "# SENTIMENT DERIVATION:\n",
    "#   votes.positive / votes.negative â†’ bullish / bearish / neutral\n",
    "#   This becomes training data for a Databricks sentiment classifier:\n",
    "#     Input:  news headline\n",
    "#     Output: bullish / bearish / neutral + confidence score\n",
    "#   Show this in MLflow Experiments during the Databricks demo moment.\n",
    "#\n",
    "# Free key at: https://cryptopanic.com/developers/api/\n",
    "# Without a key, use auth_token=\"free\" for public access.\n",
    "\n",
    "CRYPTOPANIC_KEY = os.getenv(\"CRYPTOPANIC_KEY\", \"free\")\n",
    "\n",
    "def collect_cryptopanic_news(max_pages_per_filter=40):\n",
    "    out = f\"{DIRS['news']}/cryptopanic_news.jsonl\"\n",
    "    existing = 0\n",
    "    if os.path.exists(out):\n",
    "        with open(out) as f: existing = sum(1 for _ in f)\n",
    "        if existing > 5000:\n",
    "            print(f\"  Skipping â€” already have {existing} news items\"); return out\n",
    "\n",
    "    BASE    = \"https://cryptopanic.com/api/v1/posts/\"\n",
    "    filters = [\"rising\",\"hot\",\"bullish\",\"bearish\",\"important\"]\n",
    "    articles = []\n",
    "\n",
    "    for filt in filters:\n",
    "        params   = {\"auth_token\": CRYPTOPANIC_KEY, \"filter\": filt, \"public\": \"true\"}\n",
    "        next_url = BASE\n",
    "        pages    = 0\n",
    "        while next_url and pages < max_pages_per_filter:\n",
    "            try:\n",
    "                r = requests.get(next_url, params=params, timeout=20)\n",
    "                if r.status_code == 429:\n",
    "                    print(f\"  Rate limited on {filt}, sleeping 60 s...\"); time.sleep(60); continue\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                for item in data.get(\"results\", []):\n",
    "                    vt   = item.get(\"votes\", {}) or {}\n",
    "                    pos  = vt.get(\"positive\", 0) or 0\n",
    "                    neg  = vt.get(\"negative\", 0) or 0\n",
    "                    sent = \"bullish\" if pos > neg * 1.5 else (\"bearish\" if neg > pos * 1.5 else \"neutral\")\n",
    "                    articles.append({\n",
    "                        \"id\":           item.get(\"id\"),\n",
    "                        \"title\":        item.get(\"title\"),\n",
    "                        \"published_at\": item.get(\"published_at\"),\n",
    "                        \"source\":       (item.get(\"source\") or {}).get(\"title\"),\n",
    "                        \"url\":          item.get(\"url\"),\n",
    "                        \"currencies\":   [c[\"code\"] for c in item.get(\"currencies\", [])],\n",
    "                        \"filter\":       filt,\n",
    "                        \"votes_pos\":    pos,\n",
    "                        \"votes_neg\":    neg,\n",
    "                        \"votes_imp\":    vt.get(\"important\", 0) or 0,\n",
    "                        \"sentiment\":    sent,\n",
    "                    })\n",
    "                next_url = data.get(\"next\"); params = {}; pages += 1; time.sleep(3)\n",
    "            except Exception as e:\n",
    "                log.warning(f\"  CryptoPanic ({filt}): {e}\"); break\n",
    "        print(f\"  Filter '{filt}': running total {len(articles)}\")\n",
    "\n",
    "    with open(out, \"w\") as f:\n",
    "        for a in articles: f.write(json.dumps(a) + \"\\n\")\n",
    "    print(f\"  âœ… {len(articles)} articles saved ({file_size_mb(out)} MB)\")\n",
    "    return out\n",
    "\n",
    "news_path = collect_cryptopanic_news()\n",
    "manifest[\"sections\"][\"news\"] = {\"total_mb\": dir_size_mb(DIRS[\"news\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce99e08",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 â€” Conversation Training Data (OpenAssistant)\n",
    "\n",
    "### Why this data?\n",
    "This is the **most important dataset for the ElevenLabs $7 920 prize**.\n",
    "\n",
    "ElevenLabs judges on:\n",
    "1. Natural multi-turn dialogue flow\n",
    "2. Context retention across turns\n",
    "3. **Voice-appropriate response length** (1â€“3 sentences, not a wall of text)\n",
    "\n",
    "OpenAssistant OASST1 = 161 K human conversations, quality-rated, multi-turn.  \n",
    "We filter for English + finance topics, reconstruct dialogue threads, and package them for Databricks fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 8.1 â€” OpenAssistant OASST1 Processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OASST1 dataset structure:\n",
    "#   - Stored as a message tree (not pre-formatted conversations)\n",
    "#   - Each message: {message_id, parent_id, role, text, rank, lang, ...}\n",
    "#   - rank=0 is the best human-rated response at each position\n",
    "#\n",
    "# Our pipeline:\n",
    "#   1. Filter: English, rank â‰¤ 1 (top-quality only)\n",
    "#   2. Reconstruct: walk parentâ†’child chains to get full dialogues\n",
    "#   3. Finance filter: keyword match on first user message\n",
    "#   4. Sample: keep ALL finance conversations + 5 000 general\n",
    "#      (general conversations teach natural turn-taking, greetings, etc.)\n",
    "#   5. Save: Parquet for Databricks training job\n",
    "#\n",
    "# KEY INSIGHT â€” Voice Length:\n",
    "#   OASST1 assistant replies avg 200â€“500 words.\n",
    "#   For ElevenLabs voice, optimal is 40â€“80 words.\n",
    "#   Fine-tune objective: \"respond naturally in 2â€“3 sentences max\"\n",
    "#   This \"voice condensing\" is a novel angle that impresses ElevenLabs judges.\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "def collect_openassistant():\n",
    "    out = f\"{DIRS['conversation']}/openassistant_processed.parquet\"\n",
    "    if os.path.exists(out):\n",
    "        print(\"  Skipping â€” already collected\"); return out\n",
    "\n",
    "    print(\"  Loading OASST1 from HuggingFace (~1.5 GB first download)...\")\n",
    "    ds = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n",
    "    df = ds.to_pandas()\n",
    "    print(f\"  Raw: {len(df):,} messages\")\n",
    "\n",
    "    # Step 1: quality filter\n",
    "    df = df[(df[\"lang\"] == \"en\") & (df[\"rank\"].fillna(999) <= 1)]\n",
    "    print(f\"  After quality filter: {len(df):,}\")\n",
    "\n",
    "    # Step 2: build conversation trees\n",
    "    id_to_msg = df.set_index(\"message_id\").to_dict(\"index\")\n",
    "\n",
    "    def build_chain(mid, depth=0):\n",
    "        if depth >= 8 or mid not in id_to_msg: return []\n",
    "        msg = id_to_msg[mid]\n",
    "        chain = [{\"role\": msg[\"role\"], \"text\": msg[\"text\"]}]\n",
    "        children = df[df[\"parent_id\"] == mid]\n",
    "        if not children.empty:\n",
    "            best = children.sort_values(\"rank\").iloc[0]\n",
    "            chain.extend(build_chain(best[\"message_id\"], depth + 1))\n",
    "        return chain\n",
    "\n",
    "    roots = df[df[\"parent_id\"].isna()][\"message_id\"].tolist()\n",
    "    print(f\"  Building threads from {len(roots):,} roots...\")\n",
    "\n",
    "    convs = []\n",
    "    for root_id in tqdm(roots[:20000], desc=\"  Threads\"):\n",
    "        chain = build_chain(root_id)\n",
    "        if len(chain) >= 2:\n",
    "            convs.append({\n",
    "                \"thread_id\":     root_id,\n",
    "                \"messages\":      json.dumps(chain),\n",
    "                \"num_turns\":     len(chain) // 2,\n",
    "                \"first_msg\":     chain[0][\"text\"][:300],\n",
    "            })\n",
    "\n",
    "    conv_df = pd.DataFrame(convs)\n",
    "\n",
    "    # Step 3: finance keyword filter\n",
    "    KEYWORDS = (\"bitcoin|crypto|ethereum|blockchain|defi|trading|invest|stock|\"\n",
    "                \"market|price|portfolio|finance|money|bank|asset|token|nft|\"\n",
    "                \"wallet|exchange|yield|return|risk|hedge|derivative\")\n",
    "    mask    = conv_df[\"first_msg\"].str.lower().str.contains(KEYWORDS, na=False)\n",
    "    fin_df  = conv_df[mask]\n",
    "    gen_df  = conv_df[~mask].sample(min(5000, (~mask).sum()), random_state=42)\n",
    "    final   = pd.concat([fin_df, gen_df]).reset_index(drop=True)\n",
    "    final.to_parquet(out, index=False, compression=\"snappy\")\n",
    "\n",
    "    print(f\"  Finance threads: {len(fin_df)}\")\n",
    "    print(f\"  General threads: {len(gen_df)}\")\n",
    "    print(f\"  âœ… Total: {len(final)} conversations saved ({file_size_mb(out)} MB)\")\n",
    "    return out\n",
    "\n",
    "conv_path = collect_openassistant()\n",
    "manifest[\"sections\"][\"conversation\"] = {\"total_mb\": dir_size_mb(DIRS[\"conversation\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7fc6c",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 â€” NFT & Web3 Data\n",
    "\n",
    "### Why this data?\n",
    "Completes encyclopedic coverage. Without it the agent can't handle:\n",
    "*\"What's the floor price on Bored Apes?\"* or *\"Explain ERC-721 vs ERC-1155\"*\n",
    "\n",
    "EIP standard documents also give technical depth for smart contract questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 9.1 â€” OpenSea Collections + EIP Token Standards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OpenSea data (top 500 collections by volume):\n",
    "#   name, category, total_supply, num_owners,\n",
    "#   floor_price (ETH), total_volume (ETH), market_cap (ETH)\n",
    "#\n",
    "# EIP Standards â€” critical ones for the voice agent:\n",
    "#   ERC-20   Fungible token (every altcoin uses this)\n",
    "#   ERC-721  Non-fungible token â€” defines unique ownership\n",
    "#   ERC-1155 Multi-token â€” both fungible and non-fungible in one contract\n",
    "#   ERC-2981 NFT royalty standard â€” how creators earn on secondary sales\n",
    "#   ERC-4626 Tokenized vaults â€” backbone of DeFi yield aggregators\n",
    "#   ERC-4337 Account abstraction â€” enables smart contract wallets\n",
    "#\n",
    "# We pull EIP Markdown directly from ethereum/EIPs GitHub repo.\n",
    "\n",
    "def collect_opensea(max_collections=500):\n",
    "    out = f\"{DIRS['nft']}/opensea_collections.jsonl\"\n",
    "    if os.path.exists(out):\n",
    "        print(\"  Skipping OpenSea â€” already collected\"); return out\n",
    "\n",
    "    cols = []\n",
    "    cursor = None\n",
    "    while len(cols) < max_collections:\n",
    "        params = {\"limit\": 100, \"order_by\": \"total_volume\"}\n",
    "        if cursor: params[\"next\"] = cursor\n",
    "        try:\n",
    "            r = requests.get(\"https://api.opensea.io/api/v2/collections\",\n",
    "                             params=params,\n",
    "                             headers={\"accept\": \"application/json\"},\n",
    "                             timeout=20)\n",
    "            if r.status_code == 429:\n",
    "                print(\"  Rate limited, sleeping 30 s...\"); time.sleep(30); continue\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            for c in data.get(\"collections\", []):\n",
    "                cols.append({\n",
    "                    \"collection\": c.get(\"collection\"),\n",
    "                    \"name\":       c.get(\"name\"),\n",
    "                    \"description\":str(c.get(\"description\", \"\"))[:500],\n",
    "                    \"category\":   c.get(\"category\"),\n",
    "                    \"total_supply\":c.get(\"total_supply\"),\n",
    "                    \"num_owners\": c.get(\"num_owners\"),\n",
    "                    \"floor_price\":(c.get(\"stats\") or {}).get(\"floor_price\"),\n",
    "                    \"total_volume\":(c.get(\"stats\") or {}).get(\"total_volume\"),\n",
    "                    \"market_cap\": (c.get(\"stats\") or {}).get(\"market_cap\"),\n",
    "                })\n",
    "            cursor = data.get(\"next\")\n",
    "            if not cursor: break\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            log.warning(f\"  OpenSea: {e}\"); break\n",
    "\n",
    "    with open(out, \"w\") as f:\n",
    "        for c in cols: f.write(json.dumps(c) + \"\\n\")\n",
    "    print(f\"  âœ… {len(cols)} NFT collections saved\")\n",
    "    return out\n",
    "\n",
    "def collect_eip_standards():\n",
    "    out = f\"{DIRS['nft']}/eip_standards.jsonl\"\n",
    "    if os.path.exists(out):\n",
    "        print(\"  EIP standards already collected\"); return out\n",
    "\n",
    "    EIPS = [20, 721, 1155, 2981, 4626, 4337, 4907, 1, 712, 1193, 1271, 2612, 3525]\n",
    "    records = []\n",
    "    for num in EIPS:\n",
    "        url = f\"https://raw.githubusercontent.com/ethereum/EIPs/master/EIPS/eip-{num}.md\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code == 200:\n",
    "                records.append({\"eip\": num, \"url\": url, \"content\": r.text[:5000]})\n",
    "                print(f\"  âœ… EIP-{num}\")\n",
    "        except: pass\n",
    "        time.sleep(1)\n",
    "\n",
    "    with open(out, \"w\") as f:\n",
    "        for rec in records: f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\"  âœ… {len(records)} EIP standards saved\")\n",
    "    return out\n",
    "\n",
    "collect_opensea()\n",
    "collect_eip_standards()\n",
    "manifest[\"sections\"][\"nft\"] = {\"total_mb\": dir_size_mb(DIRS[\"nft\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c596bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10 â€” TrueMarkets Live API Client\n",
    "\n",
    "### Why this section?\n",
    "This is **demo-critical**. VoiceFlow needs live market data during the presentation.\n",
    "\n",
    "We build and test the HMAC-authenticated REST client + WebSocket subscriber here so there are zero surprises on demo day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 10.1 â€” TrueX REST Client with HMAC Auth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# From TrueX REST API docs:\n",
    "#   Base URL: https://prod.truex.co/api/v1/\n",
    "#\n",
    "# HMAC signature payload: timestamp + METHOD + path + body\n",
    "# All concatenated as strings before encoding.\n",
    "#\n",
    "# Example:\n",
    "#   timestamp = \"1706000000\"\n",
    "#   payload   = \"1706000000GET/api/v1/instruments\"\n",
    "#   sig       = base64(HMAC-SHA256(api_secret, payload))\n",
    "#\n",
    "# Headers required:\n",
    "#   x-truex-auth-signature  : base64 HMAC sig\n",
    "#   x-truex-auth-token      : API key ID\n",
    "#   x-truex-auth-timestamp  : seconds since epoch\n",
    "#   x-truex-auth-userid     : client identifier\n",
    "#   X-Truex-Version         : v2026_01_23 (new envelope format)\n",
    "\n",
    "import hmac, hashlib, base64\n",
    "\n",
    "TRUEX_BASE   = \"https://prod.truex.co/api/v1\"\n",
    "TRUEX_KEY    = os.getenv(\"TRUEX_API_KEY\",    \"\")\n",
    "TRUEX_SECRET = os.getenv(\"TRUEX_API_SECRET\", \"\")\n",
    "TRUEX_UID    = os.getenv(\"TRUEX_USER_ID\",    \"\")\n",
    "\n",
    "def truex_headers(method, path, body=\"\"):\n",
    "    ts      = str(int(time.time()))\n",
    "    payload = (ts + method.upper() + path + (body or \"\")).encode(\"utf-8\")\n",
    "    sig     = base64.b64encode(\n",
    "        hmac.new(TRUEX_SECRET.encode(\"utf-8\"), payload, hashlib.sha256).digest()\n",
    "    ).decode(\"utf-8\")\n",
    "    return {\n",
    "        \"x-truex-auth-signature\": sig,\n",
    "        \"x-truex-auth-token\":     TRUEX_KEY,\n",
    "        \"x-truex-auth-timestamp\": ts,\n",
    "        \"x-truex-auth-userid\":    TRUEX_UID,\n",
    "        \"Content-Type\":           \"application/json\",\n",
    "        \"X-Truex-Version\":        \"v2026_01_23\",\n",
    "    }\n",
    "\n",
    "def truex_get(path, auth=True):\n",
    "    headers = truex_headers(\"GET\", path) if (auth and TRUEX_KEY) else {}\n",
    "    try:\n",
    "        r = requests.get(f\"{TRUEX_BASE}{path}\", headers=headers, timeout=10)\n",
    "        return r.status_code, r.json() if r.content else {}\n",
    "    except Exception as e:\n",
    "        return 0, {\"error\": str(e)}\n",
    "\n",
    "# â”€â”€ Test connectivity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Testing TrueMarkets API connectivity...\")\n",
    "for path in [\"/api/v1/health\", \"/api/v1/instruments\", \"/api/v1/market/quote\"]:\n",
    "    status, data = truex_get(path, auth=False)\n",
    "    label = \"âœ…\" if status == 200 else f\"âš ï¸  HTTP {status}\"\n",
    "    print(f\"  {label}  {path}\")\n",
    "    if status == 200:\n",
    "        fixture = f\"{DIRS['truemarkets']}/{path.split('/')[-1]}_fixture.json\"\n",
    "        with open(fixture, \"w\") as f: json.dump(data, f, indent=2)\n",
    "        print(f\"       Fixture saved: {fixture}\")\n",
    "\n",
    "manifest[\"sections\"][\"truemarkets\"] = {\"total_mb\": dir_size_mb(DIRS[\"truemarkets\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 10.2 â€” TrueX WebSocket Subscriber â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TrueX WebSocket channels (from docs):\n",
    "#   subscribe  â†’ send this to start receiving data\n",
    "#   instrument â†’ listings, status updates\n",
    "#   trade      â†’ real-time executed trades\n",
    "#   ebbo       â†’ Exchange Best Bid/Offer (top-of-book)\n",
    "#   depth      â†’ full order book depth\n",
    "#\n",
    "# UNAUTHENTICATED channels (no API key needed for market data):\n",
    "#   instrument, trade, ebbo, depth\n",
    "#\n",
    "# This cell captures a 10-second snapshot and saves it as a test fixture.\n",
    "# During the demo you would keep this connection open and feed events\n",
    "# into the VoiceFlow real-time dashboard.\n",
    "\n",
    "import asyncio, nest_asyncio\n",
    "nest_asyncio.apply()   # Allows asyncio.run() inside Jupyter\n",
    "\n",
    "TRUEX_WS_UAT  = \"wss://uat.truex.co/api/v1\"\n",
    "TRUEX_WS_PROD = \"wss://prod.truex.co/api/v1\"\n",
    "\n",
    "async def capture_ws(url=TRUEX_WS_UAT, seconds=10):\n",
    "    try:\n",
    "        import websockets\n",
    "    except ImportError:\n",
    "        print(\"  Install websockets: pip install websockets\"); return []\n",
    "\n",
    "    sub_msg = json.dumps({\n",
    "        \"type\":      \"subscribe\",\n",
    "        \"channels\":  [\"instrument\",\"ebbo\",\"trade\"],\n",
    "        \"timestamp\": int(time.time()),\n",
    "    })\n",
    "    messages = []\n",
    "    try:\n",
    "        async with websockets.connect(url, open_timeout=10) as ws:\n",
    "            await ws.send(sub_msg)\n",
    "            print(f\"  Connected to {url} â€” capturing {seconds} s...\")\n",
    "            deadline = time.time() + seconds\n",
    "            while time.time() < deadline:\n",
    "                try:\n",
    "                    raw = await asyncio.wait_for(ws.recv(), timeout=2.0)\n",
    "                    msg = json.loads(raw)\n",
    "                    msg[\"_captured_at\"] = time.time()\n",
    "                    messages.append(msg)\n",
    "                except asyncio.TimeoutError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸  WebSocket unavailable: {e}\")\n",
    "        print(\"     (Expected if running before hackathon credentials are set up)\")\n",
    "\n",
    "    out = f\"{DIRS['truemarkets']}/ws_snapshot.jsonl\"\n",
    "    with open(out, \"w\") as f:\n",
    "        for m in messages: f.write(json.dumps(m) + \"\\n\")\n",
    "    print(f\"  âœ… Captured {len(messages)} WebSocket messages\")\n",
    "    return messages\n",
    "\n",
    "asyncio.run(capture_ws())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b8a75",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11 â€” Build ChromaDB RAG Index\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "User voice â†’ ElevenLabs STT â†’ query text\n",
    "  â†’ ChromaDB semantic search â†’ top 5 chunks\n",
    "  â†’ Claude orchestrator (+ Nemotron for math)\n",
    "  â†’ response text â†’ ElevenLabs TTS â†’ voice output\n",
    "```\n",
    "\n",
    "### Why ChromaDB?\n",
    "- âœ… Fully local â€” works offline during the hackathon\n",
    "- âœ… Sub-100 ms query time â€” invisible latency in demo\n",
    "- âœ… Persistent on disk â€” survives notebook restarts; copy to USB\n",
    "- âœ… Source-filtered search â€” restrict queries to `truemarkets_official` docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 11.1 â€” Build ChromaDB Index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Embedding model: all-MiniLM-L6-v2\n",
    "#   - 384 dimensions, ~80 MB on disk\n",
    "#   - 5Ã— faster inference than OpenAI text-embedding-ada-002\n",
    "#   - Runs fully offline â€” no API calls during demo\n",
    "#   - Good performance on both technical documentation and conversational text\n",
    "#\n",
    "# INDEXING PRIORITY ORDER:\n",
    "#   1. TrueMarkets docs  â†’ highest priority; directly tied to sponsor prize\n",
    "#   2. Wikipedia         â†’ broad accessible knowledge for voice responses\n",
    "#   3. ArXiv papers      â†’ technical depth for judge probing\n",
    "#   4. CoinGecko         â†’ coin-specific factual lookups\n",
    "#   5. DeFi protocols    â†’ ecosystem coverage\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(\"Loading embedding model (downloads ~80 MB first time)...\")\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "client = chromadb.PersistentClient(path=DIRS[\"rag_index\"])\n",
    "try:    client.delete_collection(\"voiceflow_kb\")\n",
    "except: pass\n",
    "collection = client.create_collection(\n",
    "    \"voiceflow_kb\",\n",
    "    metadata={\"description\": \"VoiceFlow Intelligence knowledge base\",\n",
    "              \"built_at\": datetime.now().isoformat()}\n",
    ")\n",
    "print(\"âœ… Collection created: voiceflow_kb\")\n",
    "\n",
    "def add_docs(texts, metadatas, ids, batch=100):\n",
    "    for i in range(0, len(texts), batch):\n",
    "        t = texts[i:i+batch]; m = metadatas[i:i+batch]; d = ids[i:i+batch]\n",
    "        embs = embedder.encode(t, show_progress_bar=False).tolist()\n",
    "        collection.add(documents=t, embeddings=embs, metadatas=m, ids=d)\n",
    "\n",
    "# â”€â”€ TrueMarkets docs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "tm_file = f\"{DIRS['regulatory']}/truemarkets_docs.jsonl\"\n",
    "if os.path.exists(tm_file):\n",
    "    texts, metas, ids = [], [], []\n",
    "    with open(tm_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            doc = json.loads(line)\n",
    "            texts.append(doc[\"text\"])\n",
    "            metas.append({\"source_type\": \"truemarkets\", \"source\": doc[\"source\"]})\n",
    "            ids.append(f\"tm_{i}\")\n",
    "    add_docs(texts, metas, ids)\n",
    "    print(f\"  âœ… Indexed {len(texts)} TrueMarkets chunks\")\n",
    "\n",
    "# â”€â”€ Wikipedia â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "wiki_file = f\"{DIRS['protocols']}/wikipedia_articles.jsonl\"\n",
    "if os.path.exists(wiki_file):\n",
    "    texts, metas, ids = [], [], []\n",
    "    with open(wiki_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            doc = json.loads(line)\n",
    "            full = doc.get(\"full_text\", \"\")\n",
    "            for j in range(0, min(len(full), 30000), 900):\n",
    "                chunk = full[j:j+1000]\n",
    "                if len(chunk) > 100:\n",
    "                    texts.append(chunk)\n",
    "                    metas.append({\"source_type\": \"wikipedia\", \"title\": doc[\"title\"]})\n",
    "                    ids.append(f\"wiki_{i}_{j}\")\n",
    "    add_docs(texts, metas, ids)\n",
    "    print(f\"  âœ… Indexed {len(texts)} Wikipedia chunks\")\n",
    "\n",
    "# â”€â”€ ArXiv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "arxiv_file = f\"{DIRS['protocols']}/arxiv_papers.jsonl\"\n",
    "if os.path.exists(arxiv_file):\n",
    "    texts, metas, ids = [], [], []\n",
    "    with open(arxiv_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            doc = json.loads(line)\n",
    "            texts.append(f\"{doc['title']}\\n\\n{doc['abstract']}\")\n",
    "            metas.append({\"source_type\": \"academic\", \"arxiv_id\": doc[\"id\"]})\n",
    "            ids.append(f\"arxiv_{i}\")\n",
    "    add_docs(texts, metas, ids)\n",
    "    print(f\"  âœ… Indexed {len(texts)} ArXiv papers\")\n",
    "\n",
    "# â”€â”€ CoinGecko profiles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cg_file = f\"{DIRS['coingecko']}/coin_profiles.jsonl\"\n",
    "if os.path.exists(cg_file):\n",
    "    texts, metas, ids = [], [], []\n",
    "    with open(cg_file) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            doc = json.loads(line)\n",
    "            if not doc.get(\"description\"): continue\n",
    "            cap = doc.get(\"market_cap_usd\")\n",
    "            texts.append(\n",
    "                f\"{doc['name']} ({str(doc.get('symbol','')).upper()})\\n\"\n",
    "                f\"Categories: {', '.join(doc.get('categories', []))}\\n\"\n",
    "                f\"Market Cap: ${cap:,.0f}\\n\"  if isinstance(cap, (int,float)) else\n",
    "                f\"Market Cap: N/A\\n\"\n",
    "                f\"Algorithm: {doc.get('hashing_algorithm','N/A')}\\n\"\n",
    "                f\"{doc['description'][:800]}\"\n",
    "            )\n",
    "            metas.append({\"source_type\": \"coingecko\", \"coin_id\": doc[\"id\"]})\n",
    "            ids.append(f\"coin_{i}\")\n",
    "    add_docs(texts, metas, ids)\n",
    "    print(f\"  âœ… Indexed {len(texts)} CoinGecko profiles\")\n",
    "\n",
    "# â”€â”€ DeFi protocols â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "defi_file = f\"{DIRS['defi']}/all_protocols.parquet\"\n",
    "if os.path.exists(defi_file):\n",
    "    df_defi = pd.read_parquet(defi_file)\n",
    "    df_defi = df_defi[df_defi[\"description\"].str.len().fillna(0) > 50].head(1000)\n",
    "    texts, metas, ids = [], [], []\n",
    "    for i, row in df_defi.iterrows():\n",
    "        tvl = row.get(\"tvl\", 0)\n",
    "        texts.append(\n",
    "            f\"{row['name']} â€” {row.get('category','N/A')}\\n\"\n",
    "            f\"TVL: ${tvl:,.0f}\\nChains: {row.get('chain','N/A')}\\n\"\n",
    "            f\"{row['description']}\"\n",
    "        )\n",
    "        metas.append({\"source_type\": \"defi\", \"protocol\": row[\"name\"]})\n",
    "        ids.append(f\"defi_{i}\")\n",
    "    add_docs(texts, metas, ids)\n",
    "    print(f\"  âœ… Indexed {len(texts)} DeFi protocols\")\n",
    "\n",
    "print(f\"\\nâœ… RAG index complete â€” {collection.count():,} total documents\")\n",
    "manifest[\"sections\"][\"rag_index\"] = {\"documents\": collection.count(),\n",
    "                                      \"total_mb\": dir_size_mb(DIRS[\"rag_index\"])}\n",
    "save_manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9258b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 11.2 â€” RAG Query Function (Copy into VoiceFlow Demo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# This is the function you import in your main app.\n",
    "# Claude agent calls it as a tool to retrieve grounded context.\n",
    "\n",
    "def rag_query(query_text, n_results=5, source_filter=None):\n",
    "    \"\"\"\n",
    "    Semantic search over the VoiceFlow knowledge base.\n",
    "\n",
    "    Args:\n",
    "        query_text    : Natural language question from voice input\n",
    "        n_results     : How many chunks to retrieve (5 is optimal)\n",
    "        source_filter : Restrict to one source type:\n",
    "                        'truemarkets' | 'defi' | 'coingecko'\n",
    "                        'academic' | 'wikipedia'\n",
    "    Returns:\n",
    "        List of {text, source_type, source, distance} dicts\n",
    "        distance: 0 = perfect match, 1 = unrelated\n",
    "\n",
    "    Example Claude agent tool call:\n",
    "        context = rag_query(\"How do TrueX price bands work?\",\n",
    "                            source_filter=\"truemarkets\")\n",
    "        # Returns chunks from Rule_Book.txt and Price_Bands.txt\n",
    "    \"\"\"\n",
    "    where  = {\"source_type\": source_filter} if source_filter else None\n",
    "    emb    = embedder.encode([query_text]).tolist()\n",
    "    res    = collection.query(query_embeddings=emb, n_results=n_results,\n",
    "                              where=where,\n",
    "                              include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    return [\n",
    "        {\"text\":        doc,\n",
    "         \"source_type\": meta.get(\"source_type\"),\n",
    "         \"source\":      meta.get(\"source\") or meta.get(\"title\") or meta.get(\"coin_id\"),\n",
    "         \"distance\":    round(dist, 4)}\n",
    "        for doc, meta, dist in zip(\n",
    "            res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# â”€â”€ Smoke tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TEST_QUERIES = [\n",
    "    (\"How do TrueX price bands work?\",               \"truemarkets\"),\n",
    "    (\"What is the minimum order notional value?\",    \"truemarkets\"),\n",
    "    (\"Explain Uniswap AMM constant product formula\", \"defi\"),\n",
    "    (\"What is Bitcoin's proof of work?\",             \"wikipedia\"),\n",
    "    (\"Solana market cap and consensus mechanism\",    \"coingecko\"),\n",
    "]\n",
    "\n",
    "print(\"RAG smoke tests\\n\" + \"â”€\"*60)\n",
    "for q, filt in TEST_QUERIES:\n",
    "    results = rag_query(q, n_results=2, source_filter=filt)\n",
    "    print(f\"Q: {q}\")\n",
    "    for r in results:\n",
    "        print(f\"  [{r['source_type']}:{r['source']}] d={r['distance']} \"\n",
    "              f\"â†’ {r['text'][:90]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dc366",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12 â€” Validation Report & Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 12.1 â€” Collection Summary Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(\"  VOICEFLOW INTELLIGENCE â€” DATA COLLECTION REPORT\")\n",
    "print(f\"  Generated: {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "rows = [\n",
    "    (\"01 Market Prices\",    DIRS[\"prices\"]),\n",
    "    (\"02 CoinGecko\",        DIRS[\"coingecko\"]),\n",
    "    (\"03 DeFi Ecosystem\",   DIRS[\"defi\"]),\n",
    "    (\"04 Protocol Docs\",    DIRS[\"protocols\"]),\n",
    "    (\"05 Regulatory\",       DIRS[\"regulatory\"]),\n",
    "    (\"06 News\",             DIRS[\"news\"]),\n",
    "    (\"07 Conversations\",    DIRS[\"conversation\"]),\n",
    "    (\"08 NFT & Web3\",       DIRS[\"nft\"]),\n",
    "    (\"09 TrueMarkets Live\", DIRS[\"truemarkets\"]),\n",
    "    (\"10 RAG Index\",        DIRS[\"rag_index\"]),\n",
    "]\n",
    "\n",
    "total_mb = 0\n",
    "print(f\"\\n  {'Section':<25} {'MB':>8}   Files\")\n",
    "print(\"  \" + \"â”€\" * 42)\n",
    "for name, path in rows:\n",
    "    mb = dir_size_mb(path)\n",
    "    total_mb += mb\n",
    "    n  = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "    print(f\"  {name:<25} {mb:>8.1f}   {n}\")\n",
    "print(\"  \" + \"â”€\" * 42)\n",
    "print(f\"  {'TOTAL':<25} {total_mb:>8.1f} MB\")\n",
    "\n",
    "try:    print(f\"\\n  RAG index: {collection.count():,} documents\")\n",
    "except: pass\n",
    "\n",
    "print(\"\\n  PRIZE READINESS\")\n",
    "print(\"  \" + \"â”€\" * 42)\n",
    "checks = [\n",
    "    (\"ElevenLabs  $7 920\", os.path.exists(f\"{DIRS['conversation']}/openassistant_processed.parquet\")),\n",
    "    (\"Databricks  $3 000\", os.path.exists(f\"{DIRS['prices']}/BTCUSDT_features.parquet\")),\n",
    "    (\"TrueMarkets   $500\", os.path.exists(f\"{DIRS['regulatory']}/truemarkets_docs.jsonl\")),\n",
    "    (\"Claude/Nem  $1 000\", os.path.exists(DIRS[\"rag_index\"])),\n",
    "    (\"FinTech Trk   $500\", os.path.exists(f\"{DIRS['defi']}/all_protocols.parquet\")),\n",
    "    (\"Best Overall $2 000\", total_mb > 1000),\n",
    "]\n",
    "for prize, ok in checks:\n",
    "    print(f\"  {'âœ…' if ok else 'âŒ'}  {prize}\")\n",
    "\n",
    "manifest[\"completed_at\"] = datetime.now().isoformat()\n",
    "manifest[\"total_mb\"]     = total_mb\n",
    "save_manifest()\n",
    "print(f\"\\n  Manifest â†’ {MANIFEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ CELL 12.2 â€” Hackathon Cheat Sheet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SHEET = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘        VOICEFLOW INTELLIGENCE â€” HACKATHON CHEAT SHEET        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  BASE DIR:  {BASE_DIR}\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  KEY FILE PATHS:                                             â•‘\n",
    "â•‘  Prices:        {DIRS['prices']}\n",
    "â•‘  CoinGecko:     {DIRS['coingecko']}\n",
    "â•‘  DeFi:          {DIRS['defi']}\n",
    "â•‘  RAG Index:     {DIRS['rag_index']}\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  TRUEX REST API:   https://prod.truex.co/api/v1/             â•‘\n",
    "â•‘  TRUEX WS PROD:    wss://prod.truex.co/api/v1                â•‘\n",
    "â•‘  TRUEX WS UAT:     wss://uat.truex.co/api/v1                 â•‘\n",
    "â•‘  Auth:  HMAC-SHA256(timestamp + METHOD + path + body)        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  KEY ENDPOINTS:                                              â•‘\n",
    "â•‘  GET  /api/v1/health           â†’ service health              â•‘\n",
    "â•‘  GET  /api/v1/instruments      â†’ available trading pairs     â•‘\n",
    "â•‘  GET  /api/v1/market/quote     â†’ live EBBO                   â•‘\n",
    "â•‘  POST /api/v1/orders           â†’ create order                â•‘\n",
    "â•‘  GET  /api/v1/orders/active    â†’ open orders                 â•‘\n",
    "â•‘  GET  /api/v1/balances         â†’ account balances            â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  RAG USAGE:                                                  â•‘\n",
    "â•‘  results = rag_query(                                        â•‘\n",
    "â•‘      \"How do price bands work?\",                             â•‘\n",
    "â•‘      source_filter=\"truemarkets\"                             â•‘\n",
    "â•‘  )                                                           â•‘\n",
    "â•‘  context = \"\\n\".join(r[\"text\"] for r in results)             â•‘\n",
    "â•‘  # Inject context into Claude agent system prompt            â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  PRIZES:  ElevenLabs $7 920 | Databricks $3 000              â•‘\n",
    "â•‘           Claude $1 000     | TrueMarkets $500               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "print(SHEET)\n",
    "with open(f\"{BASE_DIR}/CHEAT_SHEET.txt\", \"w\") as f:\n",
    "    f.write(SHEET)\n",
    "print(f\"Saved to: {BASE_DIR}/CHEAT_SHEET.txt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
